<?xml version="1.0"?>
<Container version="2">
  <Name>tdarr_node</Name>
  <Repository>haveagitgat/tdarr_node</Repository>
  <Registry>https://hub.docker.com/r/haveagitgat/tdarr_node/</Registry>
  <Network>host</Network>
  <MyIP/>
  <Shell>sh</Shell>
  <Privileged>false</Privileged>
  <Support>https://forums.unraid.net/topic/84070-support-haveagitgat-tdarr-audiovideo-library-analytics-transcode-automation/</Support>
  <Project>https://github.com/HaveAGitGat/Tdarr</Project>
  <Overview>(tdarr server required separately) Tdarr V2 is a distributed transcoding system for automating media library transcode/remux management and making sure your files are exactly how you need them to be in terms of codecs/streams/containers and so on. Put your spare hardware to use with Tdarr Nodes for Windows, Linux (including Linux arm) and macOS. &#xD;
[br][br]&#xD;
Designed to work alongside applications like Sonarr/Radarr and built with the aim of modularisation, parallelisation and scalability, each library you add has its own transcode settings, filters and schedule. Workers can be fired up and closed down as necessary, and are split into 4 types - Transcode CPU/GPU and Health Check CPU/GPU. Worker limits can be managed by the scheduler as well as manually. &#xD;
[br][br]&#xD;
For a desktop application with similar functionality please see HBBatchBeast.&#xD;
[br][br]&#xD;
Docs here: https://tdarr.io/docs/&#xD;
[br][br]&#xD;
Plugins here: https://github.com/HaveAGitGat/Tdarr_Plugins&#xD;
[br][br]&#xD;
&#xD;
  </Overview>
  <Category>Productivity: MediaApp:Video</Category>
  <WebUI/>
  <Icon>https://github.com/ground7/docker-templates/blob/master/img/tdarr-node.png?raw=true</Icon>
  <ExtraParams>--runtime=nvidia</ExtraParams>
  <PostArgs/>
  <CPUset/>
  <DateInstalled>1612251504</DateInstalled>
  <Description>(tdarr server required separately) Tdarr V2 is a distributed transcoding system for automating media library transcode/remux management and making sure your files are exactly how you need them to be in terms of codecs/streams/containers and so on. Put your spare hardware to use with Tdarr Nodes for Windows, Linux (including Linux arm) and macOS. &#xD;
[br][br]&#xD;
Designed to work alongside applications like Sonarr/Radarr and built with the aim of modularisation, parallelisation and scalability, each library you add has its own transcode settings, filters and schedule. Workers can be fired up and closed down as necessary, and are split into 4 types - Transcode CPU/GPU and Health Check CPU/GPU. Worker limits can be managed by the scheduler as well as manually. &#xD;
[br][br]&#xD;
For a desktop application with similar functionality please see HBBatchBeast.&#xD;
[br][br]&#xD;
Docs here: https://tdarr.io/docs/&#xD;
[br][br]&#xD;
Plugins here: https://github.com/HaveAGitGat/Tdarr_Plugins&#xD;
[br][br]&#xD;
&#xD;
  </Description>
  <Networking>
    <Mode>host</Mode>
    <Publish>
      <Port>
        <HostPort>8266</HostPort>
        <ContainerPort>8266</ContainerPort>
        <Protocol>tcp</Protocol>
      </Port>
      <Port>
        <HostPort>8267</HostPort>
        <ContainerPort>8267</ContainerPort>
        <Protocol>tcp</Protocol>
      </Port>
    </Publish>
  </Networking>
  <Data>
    <Volume>
      <HostDir>/mnt/user/appdata/tdarr_node/config</HostDir>
      <ContainerDir>/app/configs</ContainerDir>
      <Mode>rw</Mode>
    </Volume>
    <Volume>
      <HostDir>/mnt/user/appdata/tdarr_node/logs</HostDir>
      <ContainerDir>/app/logs</ContainerDir>
      <Mode>rw</Mode>
    </Volume>
    <Volume>
      <HostDir>/mnt/user/media/local/</HostDir>
      <ContainerDir>/media</ContainerDir>
      <Mode>rw</Mode>
    </Volume>
    <Volume>
      <HostDir>/mnt/user/temp/tdarr_transcode/</HostDir>
      <ContainerDir>/temp</ContainerDir>
      <Mode>rw</Mode>
    </Volume>
  </Data>
  <Environment>
    <Variable>
      <Value>0.0.0.0</Value>
      <Name>serverIP</Name>
      <Mode/>
    </Variable>
    <Variable>
      <Value>0.0.0.0</Value>
      <Name>nodeIP</Name>
      <Mode/>
    </Variable>
    <Variable>
      <Value>99</Value>
      <Name>PUID</Name>
      <Mode/>
    </Variable>
    <Variable>
      <Value>100</Value>
      <Name>PGID</Name>
      <Mode/>
    </Variable>
    <Variable>
      <Value>GPU-01981fdf-d4b2-4e74-ed8c-c3dcb8629146</Value>
      <Name>NVIDIA_VISIBLE_DEVICES</Name>
      <Mode/>
    </Variable>
    <Variable>
      <Value>all</Value>
      <Name>NVIDIA_DRIVER_CAPABILITIES</Name>
      <Mode/>
    </Variable>
    <Variable>
      <Value>LoyalSlaveNode</Value>
      <Name>nodeID</Name>
      <Mode/>
    </Variable>
  </Environment>
  <Labels/>
</Container>
